## Standalone-HA高可用集群模式

### 原理

![image-20211027161656568](.\img\image-20211027161656568.png)

### 步骤

1.集群规划

\- 服务器: master(Master + Slave): JobManager + TaskManager

\- 服务器: slave1(Master + Slave): JobManager + TaskManager

\- 服务器: slave2(Slave): TaskManager

2.启动ZooKeeper

```shell
zkServer.sh status
zkServer.sh stop
zkServer.sh start
# 三个服务器要同时打开！！
```

3.启动HDFS

```shell
start-dfs.sh
```

4.停止Flink集群

```shell
stop-cluster.sh
```

5.修改flink-conf.yaml

```shell
vi /software/flink/conf/flink-conf.yaml
```



增加如下内容

```shell
state.backend: filesystem
state.backend.fs.checkpointdir: hdfs://master:9000/flink-checkpoints
high-availability: zookeeper
high-availability.storageDir: hdfs://master:9000/flink/ha/
high-availability.zookeeper.quorum: master:2181,slave1:2181,slave2:2181
```

6.修改masters

```shell
vi /software/flink/conf/masters

master:8081
slave1:8081
```

7.同步

```shell
scp -r /software/flink/conf/flink-conf.yaml slave1:/software/flink/conf/
scp -r /software/flink/conf/flink-conf.yaml slave2:/software/flink/conf/
scp -r /software/flink/conf/masters slave1:/software/flink/conf/
scp -r /software/flink/conf/masters slave2:/software/flink/conf/
```

8.修改slave1上的flink-conf.yaml

```shell
vi /software/flink/conf/flink-conf.yaml

jobmanager.rpc.address: slave1
```

9.重新启动Flink集群,master上执行

```shell
stop-cluster.sh
start-cluster.sh
```

![image-20211027172349791](.\img\image-20211027172349791.png)

可以在http://master:8081/#/overview和http://slave1:8081/#/overview查看

### 测试

1. 执行wc

```shell
flink run  /software/flink/examples/batch/WordCount.jar
# 可正常运行
```

2. kill其中一个master(此处kill的master节点)

![image-20211028102253213](.\img\image-20211028102253213.png)

![image-20211028102356623](.\img\image-20211028102356623.png)

![image-20211028111352419](.\img\image-20211028111352419.png)

3. 再次运行wc

```shell
flink run  /software/flink/examples/batch/WordCount.jar
# 依旧可正常运行，测试完毕
```

## Flink-On-Yarn

-1.Yarn的资源可以按需使用，提高集群的资源利用率
-2.Yarn的任务有优先级，根据优先级运行作业
-3.基于Yarn调度系统，能够自动化地处理各个角色的 Failover(容错)
○ JobManager 进程和 TaskManager 进程都由 Yarn NodeManager 监控
○ 如果 JobManager 进程异常退出，则 Yarn ResourceManager 会重新调度 JobManager 到其他机器
○ 如果 TaskManager 进程异常退出，JobManager 会收到消息并重新向 Yarn ResourceManager 申请资源，重新启动 TaskManager

![1610591841303](.\img\1610591841303.png)

### 两种模式

#### Session会话模式

![1610592015551](.\img\1610592015551.png)

#### Job分离模式

![1610592128486](.\img\1610592128486.png)

### 步骤

```shell
# 关闭yarn的内存检查(spark学习里已经关闭，这里检查)

vi /software/hadoop/etc/hadoop/yarn-site.xml
```

```xml
 <!-- 关闭yarn内存检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
```

若是更改需要同步分发到其他节点并重启yarn

### 测试

#### Session会话模式

在Yarn上启动一个Flink集群,并重复使用该集群,后续提交的任务都是给该集群,资源会被一直占用,除非手动关闭该集群----适用于大量的小任务

1. 在yarn上启动一个Flink集群/会话，master上执行以下命令

```shell
/software/flink/bin/yarn-session.sh -n 2 -tm 800 -s 1 -d
# 申请2个CPU、1600M内存
# -n 表示申请2个容器，这里指的就是多少个taskmanager
# -tm 表示每个TaskManager的内存大小
# -s 表示每个TaskManager的slots数量
# -d 表示以后台程序方式运行
```

2. http://master:8088/cluster

![image-20211028111818556](.\img\image-20211028111818556.png)

3. 使用flink run提交任务：

```shell
flink run  /software/flink/examples/batch/WordCount.jar
# 运行完之后可以继续运行其他的小任务

flink run  /software/flink/examples/batch/WordCount.jar
```

4. 通过上方的ApplicationMaster可以进入Flink的管理界面

![image-20211028164639930](.\img\image-20211028164639930.png)

5. 关闭yarn-session：

```shell
yarn application -kill application_1635390066517_0001	
```

#### Job分离模式--用的更多

针对每个Flink任务在Yarn上启动一个独立的Flink集群并运行,结束后自动关闭并释放资源,----适用于大任务

```shell
flink run -m yarn-cluster -yjm 1024 -ytm 1024 /software/flink/examples/batch/WordCount.jar
# -m  jobmanager的地址
# -yjm 1024 指定jobmanager的内存信息
# -ytm 1024 指定taskmanager的内存信息
```

小插曲

![image-20211028171507320](.\img\image-20211028171507320.png)

```shell
vi /software/flink/conf/flink-conf.yaml 
# 添加以下内容
classloader.check-leaked-classloader: false
# 解决
```





