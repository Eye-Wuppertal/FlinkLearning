1.数据来源：比如，网站或者app。非常重要的一点，就是埋点。也就是说，埋点，在网站/app的哪个页面的哪些操作发生时，前端的代码（网站，JavaScript；app，android/IOS），就通过网络请求，（Ajax；socket），向后端的服务器发送指定格式的日志数据。

2.Nginx，后台Web服务器（Tomcat、Jetty），后台系统（J2EE、PHP）。到这一步为止，其实还是可以跟我们之前的离线日志收集流程一样。走后面的通过一个日志传输工具，给放入指定的文件夹。
flume，监控指定的文件夹

3.Kafka，我们的日志数据，怎么处理，都是由你自己决定。可以每天收集一份，放到flume，转移到HDFS里面，清洗后放入Hive，建立离线的数据仓库。

也可以每收集1分钟的数据，或者每收集一点数据，就放入文件，然后转移到flume中去，或者直接通过API定制，直接把一条一条的log打入flume。可以配置flume，将数据写入Kafka

4.实时数据，通常都是从分布式消息队列集群中读取的，比如Kafka；实时数据，实时的log，实时的写入到消息队列中，比如Kafka；然后呢，再由我们后端的实时数据处理程序（Storm、Spark Streaming），实时从Kafka中读取数据，log日志。然后进行实时的计算和处理。

5.实时的，主动从Kafka中拉取数据

6.大数据实时计算系统，比如说用Storm、Spark Streaming、Flink开发的，可以实时的从Kafka中拉取数据，然后对实时的数据进行处理和计算，这里可以封装大量复杂的业务逻辑，甚至调用复杂的机器学习、数据挖掘、智能推荐的算法，然后实现实时的车辆调度、实时推荐。
————————————————
版权声明：本文为CSDN博主「码动乾坤」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/longwenyanlan/article/details/84557378



![image-20211130172705199](.\img\image-20211130172705199.png)

大数据项目工程第一步

数据采集，一般用户的行为都会记录在日志中或者业务交互直接存到SQL里，然后对日志或SQL进行数据读取，存入HDFS中，后续再进行数据分析



